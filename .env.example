# Local Model Configuration

# Embedding Model (HuggingFace model)
# This model will be downloaded automatically on the first run if not available locally.
# Recommended multilingual model for broader language support:
EMBEDDING_MODEL_NAME=sentence-transformers/all-MiniLM-L6-v2
# Other options (e.g., for specific languages or performance):
# EMBEDDING_MODEL_NAME=BAAI/bge-small-en-v1.5

# LLM Configuration (using Ollama local service)
# Ensure Ollama is installed and running: https://ollama.ai/
LLM_MODEL_NAME=qwen3:4b
# Other LLM options (e.g., smaller models for less resource usage):
# LLM_MODEL_NAME=llama2:7b
OLLAMA_BASE_URL=http://localhost:11434

# Vector Database Configuration (FAISS - pure Python version, no C++ compilation required)
FAISS_PERSIST_DIRECTORY=./data/vectorstore
INDEX_NAME=faiss_index

# Document Processing Configuration
CHUNK_SIZE=500
CHUNK_OVERLAP=50
MAX_DOCUMENTS=1000

# RAG Retrieval Configuration
RETRIEVAL_TOP_K=5
TEMPERATURE=0.7
MAX_TOKENS=2000

# Logging Configuration
LOG_LEVEL=INFO
LOG_FILE=./logs/rag_system.log